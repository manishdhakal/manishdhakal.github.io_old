<!DOCTYPE html>
<html lang="en">

  <!-- Head -->
  <head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">    <!-- Metadata, OpenGraph and Schema.org -->
    

    <!-- Standard metadata -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <title>Publications | Manish  Dhakal</title>
    <meta name="author" content="Manish  Dhakal">
    <meta name="description" content="Portfolio website of Manish Dhakal.
">
    <meta name="keywords" content="manish, manish-dhakal, academic-website, portfolio-website, ai-researcher">


    <!-- Bootstrap & MDB -->
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous">

    <!-- Bootstrap Table -->
    <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css">

    <!-- Fonts & Icons -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@5.15.4/css/all.min.css" integrity="sha256-mUZM63G8m73Mcidfrv5E+Y61y7a12O5mW4ezU3bxqW4=" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous">
    <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

    <!-- Code Syntax Highlighting -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light">

    

    <!-- Styles -->
    <link rel="shortcut icon" href="/assets/img/favicon.png">
    <link rel="stylesheet" href="/assets/css/main.css">
    <link rel="canonical" href="https://manishdhakal.github.io/publications/">

    <!-- Dark Mode -->
    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark">

    <script src="/assets/js/theme.js"></script>
    <script src="/assets/js/dark_mode.js"></script>
    

  </head>

  <!-- Body -->
  <body class="fixed-top-nav ">

    <!-- Header -->
    <header>

      <!-- Nav Bar -->
      <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
        <div class="container">
          <a class="navbar-brand title font-weight-lighter" href="/"><span class="font-weight-bold">Manish </span>Dhakal</a>
          <!-- Navbar Toggle -->
          <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
            <span class="sr-only">Toggle navigation</span>
            <span class="icon-bar top-bar"></span>
            <span class="icon-bar middle-bar"></span>
            <span class="icon-bar bottom-bar"></span>
          </button>

          <div class="collapse navbar-collapse text-right" id="navbarNav">
            <ul class="navbar-nav ml-auto flex-nowrap">

              <!-- About -->
              <li class="nav-item ">
                <a class="nav-link" href="/">About</a>
              </li>
              

              <!-- Other pages -->
              <li class="nav-item active">
                <a class="nav-link" href="/publications/">Publications<span class="sr-only">(current)</span></a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/repositories/">Repositories</a>
              </li>
              <li class="nav-item ">
                <a class="nav-link" href="/cv/">CV</a>
              </li>

              <!-- Toogle theme mode -->
              <li class="toggle-container">
                <button id="light-toggle" title="Change theme">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
                </button>
              </li>
            </ul>
          </div>
        </div>
      </nav>

      <!-- Scrolling Progress Bar -->
      <progress id="progress" value="0">
        <div class="progress-container">
          <span class="progress-bar"></span>
        </div>
      </progress>
    </header>


    <!-- Content -->
    <div class="container mt-5">
      
        <!-- page.html -->
        <div class="post">

          <header class="post-header">
            <h1 class="post-title">Publications</h1>
            <p class="post-description"></p>
          </header>

          <article>
            <!-- _pages/publications.md -->
<div class="publications">
  <h2 class="year">2023</h2>
  <ol class="bibliography">
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">MICCAI</abbr></div>

        <!-- Entry bib key -->
        <div id="adhikari2023synthetic" class="col-sm-8">
        <!-- Title -->
        <div class="title">Synthetic Boost: Leveraging Synthetic Data for Enhanced Vision-Language Segmentation in Echocardiography</div>
        <!-- Author -->
        <div class="author">
        

        Rabin Adhikari, <em>Manish Dhakal</em>, Safal Thapaliya, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Kanchan Poudel, Prasiddha Bhandari, Bishesh Khanal' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2309.12829</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2309.12829" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="/assets/pdf/synthetic_boost.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/naamiinepal/synthetic-boost" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.48550/arXiv.2309.12829"></span>
              <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.48550/arXiv.2309.12829" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Accurate segmentation is essential for echocardiography-based assessment of cardiovascular diseases (CVDs). 
    However, the variability among sonographers and the inherent challenges of ultrasound images hinder precise segmentation. 
    By leveraging the joint representation of image and text modalities, Vision-Language Segmentation Models (VLSMs) can incorporate rich contextual information, potentially aiding in accurate and explainable segmentation. 
    However, the lack of readily available data in echocardiography hampers the training of VLSMs. 
    In this study, we explore using synthetic datasets from Semantic Diffusion Models (SDMs) to enhance VLSMs for echocardiography segmentation. 
    We evaluate results for two popular VLSMs (CLIPSeg and CRIS) using seven different kinds of language prompts derived from several attributes, automatically extracted from echocardiography images, segmentation masks, and their metadata. 
    Our results show improved metrics and faster convergence when pretraining VLSMs on SDM-generated synthetic images before finetuning on real images. 
    The code and prompts are released at https://github.com/naamiinepal/synthetic-boost</p>
          </div>
        </div>
      </div>
</li>
<li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">arXiv</abbr></div>

        <!-- Entry bib key -->
        <div id="poudel2023exploring" class="col-sm-8">
        <!-- Title -->
        <div class="title">Exploring Transfer Learning in Medical Image Segmentation using Vision-Language Models</div>
        <!-- Author -->
        <div class="author">
        

        Kanchan Poudel, <em>Manish Dhakal</em>, Prasiddha Bhandari, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Rabin Adhikari, Safal Thapaliya, Bishesh Khanal' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>arXiv preprint arXiv:2308.07706</em>, 2023
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="http://arxiv.org/abs/2308.07706" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">arXiv</a>
            <a href="/assets/pdf/medvlsm.pdf" class="btn btn-sm z-depth-0" role="button">PDF</a>
            <a href="https://github.com/naamiinepal/medvlsm" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.48550/arXiv.2308.07706"></span>
              <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.48550/arXiv.2308.07706" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>Medical image segmentation with deep learning is an important and widely studied topic because segmentation enables quantifying target structure size and shape that can help in disease diagnosis, prognosis, surgery planning, and understanding. 
    Recent advances in the foundation Vision-Language Models (VLMs) and their adaptation to segmentation tasks in natural images with Vision-Language Segmentation Models (VLSMs) have opened up a unique opportunity to build potentially powerful segmentation models for medical images that enable providing helpful information via language prompt as input, leverage the extensive range of other medical imaging datasets by pooled dataset training, adapt to new classes, and be robust against out-of-distribution data with human-in-the-loop prompting during inference. 
    Although transfer learning from natural to medical images for imageonly segmentation models has been studied, no studies have analyzed how the joint representation of vision-language transfers to medical images in segmentation problems and understand gaps in leveraging their full potential. 
    We present the first benchmark study on transfer learning of VLSMs to 2D medical images with thoughtfully collected 11 existing 2D medical image datasets of diverse modalities with carefully presented 9 types of language prompts from 14 attributes. 
    Our results indicate that VLSMs trained in natural image-text pairs transfer reasonably to the medical domain in zero-shot settings when prompted appropriately for non-radiology photographic modalities; when finetuned, they obtain comparable performance to conventional architectures, even in X-rays and ultrasound modalities. 
    However, the additional benefit of language prompts during finetuning may be limited, with image features playing a more dominant role; they can better handle training on pooled datasets combining diverse modalities and are potentially more robust to domain shift than the conventional segmentation models. 
    The code and prompts are released at https://github.com/naamiinepal/medvlsm</p>
          </div>
        </div>
      </div>
</li>
</ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li>
<!-- _layouts/bib.html -->
      <div class="row">
        <div class="col-sm-2 abbr"><abbr class="badge">IEEE</abbr></div>

        <!-- Entry bib key -->
        <div id="dhakal2022automatic" class="col-sm-8">
        <!-- Title -->
        <div class="title">Automatic speech recognition for the Nepali language using CNN, bidirectional LSTM and ResNet</div>
        <!-- Author -->
        <div class="author">
        

        <em>Manish Dhakal</em>, Arman Chhetri, Aman Kumar Gupta, and
          <span class="more-authors" title="click to view 3 more authors" onclick="
                var element = $(this);
                element.attr('title', '');
                var more_authors_text = element.text() == '3 more authors' ? 'Prabin Lamichhane, Suraj Pandey, Subarna Shakya' : '3 more authors';
                var cursorPosition = 0;
                var textAdder = setInterval(function(){
                  element.text(more_authors_text.substring(0, cursorPosition + 1));
                  if (++cursorPosition == more_authors_text.length){
                    clearInterval(textAdder);
                  }
              }, '10');
              ">3 more authors</span>
</div>

        <!-- Journal/Book title and date -->
        
        
        <div class="periodical">
          <em>In 2022 International Conference on Inventive Computation Technologies (ICICT)</em>, 2022
        </div>
        <div class="periodical">
          
        </div>

          <!-- Links/Buttons -->
          <div class="links">
            <a class="abstract btn btn-sm z-depth-0" role="button">Abs</a>
            <a href="https://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Code</a>
            <a href="https://ieeexplore.ieee.org/abstract/document/9850832" class="btn btn-sm z-depth-0" role="button" rel="external nofollow noopener" target="_blank">Website</a>
          </div>
          
          <div class="badges">
            <span class="altmetric-embed" data-hide-no-mentions="true" data-hide-less-than="15" data-badge-type="2" data-badge-popover="right" data-doi="https://doi.org/10.1109/ICICT54344.2022.9850832"></span>
              <span class="__dimensions_badge_embed__" data-doi="https://doi.org/10.1109/ICICT54344.2022.9850832" data-hide-zero-citations="true" data-style="small_rectangle" data-legend="hover-right" style="margin-bottom: 3px;"></span>
          </div>

          <!-- Hidden abstract block -->
          <div class="abstract hidden">
            <p>This paper presents an end-to-end deep learning model for Automatic Speech Recognition (ASR) that transcribes Nepali speech to text. 
    The model was trained and tested on the OpenSLR (audio, text) dataset. 
    The majority of the audio dataset have silent gaps at both ends which are clipped during dataset preprocessing for a more uniform mapping of audio frames and their corresponding texts. 
    Mel Frequency Cepstral Coe cients (MFCCs) are used as audio features to feed into the model. 
    The model having Bidirectional LSTM paired with ResNet and one-dimensional CNN produces the best results for this dataset out of all the models (neural networks with variations of LSTM, GRU, CNN, and ResNet) that have been trained so far. 
    This novel model uses Connectionist Temporal Classification (CTC) function for loss calculation during training and CTC beam search decoding for predicting characters as the most likely sequence of Nepali text. 
    On the test dataset, the character error rate (CER) of 17.06 percent has been achieved.
    The code and prompts are released at https://github.com/manishdhakal/ASR-Nepali-using-CNN-BiLSTM-ResNet.</p>
          </div>
        </div>
      </div>
</li></ol>


</div>

          </article>

        </div>

      
    </div>

    <!-- Footer -->    
    <footer class="fixed-bottom">
      <div class="container mt-0">
        © Copyright 2023 Manish  Dhakal. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>.

      </div>
    </footer>

    <!-- JavaScripts -->
    <!-- jQuery -->
  <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script>

    <!-- Bootsrap & MDB scripts -->
  <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script>
  <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script>

    <!-- Masonry & imagesLoaded -->
  <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script>
  <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
  <script defer src="/assets/js/masonry.js" type="text/javascript"></script>
    
  <!-- Medium Zoom JS -->
  <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script>
  <script defer src="/assets/js/zoom.js"></script>

  <!-- Bootstrap Table -->
  <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script>

  <!-- Load Common JS -->
  <script src="/assets/js/no_defer.js"></script>
  <script defer src="/assets/js/common.js"></script>
  <script defer src="/assets/js/copy_code.js" type="text/javascript"></script>

    
  <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script>
  <script async src="https://badge.dimensions.ai/badge.js"></script>

    <!-- MathJax -->
  <script type="text/javascript">
    window.MathJax = {
      tex: {
        tags: 'ams'
      }
    };
  </script>
  <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
  <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>

    
    

<!-- Scrolling Progress Bar -->
<script type="text/javascript">
  /*
   * This JavaScript code has been adapted from the article 
   * https://css-tricks.com/reading-position-indicator/ authored by Pankaj Parashar, 
   * published on the website https://css-tricks.com on the 7th of May, 2014.
   * Couple of changes were made to the original code to make it compatible 
   * with the `al-foio` theme.
   */
  const progressBar = $("#progress");
  /*
   * We set up the bar after all elements are done loading.
   * In some cases, if the images in the page are larger than the intended
   * size they'll have on the page, they'll be resized via CSS to accomodate
   * the desired size. This mistake, however, breaks the computations as the
   * scroll size is computed as soon as the elements finish loading.
   * To account for this, a minimal delay was introduced before computing the
   * values.
   */
  window.onload = function () {
    setTimeout(progressBarSetup, 50);
  };
  /*
   * We set up the bar according to the browser.
   * If the browser supports the progress element we use that.
   * Otherwise, we resize the bar thru CSS styling
   */
  function progressBarSetup() {
    if ("max" in document.createElement("progress")) {
      initializeProgressElement();
      $(document).on("scroll", function() {
        progressBar.attr({ value: getCurrentScrollPosition() });
      });
      $(window).on("resize", initializeProgressElement);
    } else {
      resizeProgressBar();
      $(document).on("scroll", resizeProgressBar);
      $(window).on("resize", resizeProgressBar);
    }
  }
  /*
   * The vertical scroll position is the same as the number of pixels that
   * are hidden from view above the scrollable area. Thus, a value > 0 is
   * how much the user has scrolled from the top
   */
  function getCurrentScrollPosition() {
    return $(window).scrollTop();
  }

  function initializeProgressElement() {
    let navbarHeight = $("#navbar").outerHeight(true);
    $("body").css({ "padding-top": navbarHeight });
    $("progress-container").css({ "padding-top": navbarHeight });
    progressBar.css({ top: navbarHeight });
    progressBar.attr({
      max: getDistanceToScroll(),
      value: getCurrentScrollPosition(),
    });
  }
  /*
   * The offset between the html document height and the browser viewport
   * height will be greater than zero if vertical scroll is possible.
   * This is the distance the user can scroll
   */
  function getDistanceToScroll() {
    return $(document).height() - $(window).height();
  }

  function resizeProgressBar() {
    progressBar.css({ width: getWidthPercentage() + "%" });
  }
  // The scroll ratio equals the percentage to resize the bar
  function getWidthPercentage() {
    return (getCurrentScrollPosition() / getDistanceToScroll()) * 100;
  }
</script>

  </body>
</html>
